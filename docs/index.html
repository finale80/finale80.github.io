<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <title>Alessandro Finamore | Home</title>
  <meta property="og:title" content="Alessandro Finamore | Home" />
  <meta property="og:image" content="https://afinamore.io/img/profile_pic_round.png" />
  <meta name="description" content="">
  <meta property="og:description" content="" />
  <meta name="author" content="Alessandro Finamore">
  <meta name="google-site-verification" content="hwUvKQPFM4AMHshyoBJpaBF1OY_SHiPHAX3H20vXqTY" />
  <link rel="shortcut icon" href="/favicon/favicon.ico">
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">
  
  <link rel="preload" href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900&display=swap" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900&display=swap"></noscript>
  <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i&display=swap" as="style" onload="this.onload=null;this.rel='stylesheet'">
      <noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i&display=swap"></noscript>
  <link rel="preload" href="https://use.fontawesome.com/releases/v5.12.1/css/all.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.1/css/all.css"></noscript>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/devicons/1.8.0/css/devicons.min.css" integrity="sha512-JW3fT0YTK7pT7w437SoX6GcW76jOZ6E0jGmrqBAcloC4GKT+njHOY4fX5KxJ9WfIXTkNrAF994525fAHp+KCxg==" crossorigin="anonymous" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/devicons/1.8.0/css/devicons.min.css" integrity="sha512-JW3fT0YTK7pT7w437SoX6GcW76jOZ6E0jGmrqBAcloC4GKT+njHOY4fX5KxJ9WfIXTkNrAF994525fAHp+KCxg==" crossorigin="anonymous"></noscript>
  <link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/simple-line-icons/2.5.5/css/simple-line-icons.min.css" integrity="sha512-QKC1UZ/ZHNgFzVKSAhV5v5j73eeL9EEN289eKAEFaAjgAiobVAnVv/AGuPbXsKl1dNoel3kNr6PYnSiTzVVBCw==" crossorigin="anonymous" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/simple-line-icons/2.5.5/css/simple-line-icons.min.css" integrity="sha512-QKC1UZ/ZHNgFzVKSAhV5v5j73eeL9EEN289eKAEFaAjgAiobVAnVv/AGuPbXsKl1dNoel3kNr6PYnSiTzVVBCw==" crossorigin="anonymous"></noscript>
  
  
  
<link rel="stylesheet" href="https://afinamore.io/sass/main.min.fb68689a5d4e876bb1023640caaceef0175cc14036e94a308d4704dda2ec7603.css">

  <link rel="preload" href="https://afinamore.io/css/tweaks.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="https://afinamore.io/css/tweaks.css"></noscript>
  <link rel="preload" href="https://afinamore.io/css/resume-override.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="https://afinamore.io/css/resume-override.css"></noscript>
  <meta name="generator" content="Hugo 0.100.1" />
  
   
  
  <script src="https://kit.fontawesome.com/888c253f21.js" crossorigin="anonymous"></script>
</head>
<body id="page-top">
  <nav class="navbar navbar-expand-lg" id="navbar">
  
  <a href="/"><img src="/favicon/favicon_exagon_light.svg" id="logo" class="navbar-grand logo-fullsize"></a>

  
  <div id="avatar"><img src="/img/me/avatar_round.png"/></div>

  
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"><i class="fas fa-bars fa-lg"></i></span>
  </button>

  <div class="collapse navbar-collapse" id="navbarSupportedContent">
    <ul class="navbar-nav mr-auto">
        <li class="nav-item"><a href="/#home" class="navbar-link">Home</a></li>
        <li class="nav-item"><a href="/#projects" class="navbar-link">Projects</a></li>
        <li class="nav-item"><a href="/#talks" class="navbar-link">Talks</a></li>
        <li class="nav-item"><a href="/#research" class="navbar-link">Research</a></li>
        <li class="nav-item"><a href="/#experience" class="navbar-link">Experience</a></li>
        <li class="nav-item"><a href="/#education" class="navbar-link">Education</a></li>
   </ul>
  </div>

  
  <div id="navbar-right" class="navbar-right-fullsize">
    
    
        <span class="navbar-link-marker" id="navbar-link-home">•</span><a href="/#home">Home</a>
        <span class="navbar-link-marker" id="navbar-link-projects">•</span><a href="/#projects">Projects</a>
        <span class="navbar-link-marker" id="navbar-link-talks">•</span><a href="/#talks">Talks</a>
        <span class="navbar-link-marker" id="navbar-link-research">•</span><a href="/#research">Research</a>
        <span class="navbar-link-marker" id="navbar-link-experience">•</span><a href="/#experience">Experience</a>
        <span class="navbar-link-marker" id="navbar-link-education">•</span><a href="/#education">Education</a>
    
  </div>
</nav>

  <div class="container-fluid p-0">
    
   
      <section class="resume-section p-3 p-lg-5 d-flex d-column" id="home">
        <div class="my-auto">
          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="mr-auto">

              <h1 class="mb-0 text-noprimary">Alessandro
                <span class="text-primary">Finamore</span>
              </h1>
              <div class="subheading mb-5">Paris, FRANCE 
               
                ·
                <a href="mailto:mail@afinamore.io">mail@afinamore.io</a>
               
              </div>

            </div>

            
        </div>
        <div>
          <p>I am a researcher working in <em>Internet measurements</em> at the intersection between <em>Deep Learning</em>, <em>BigData</em> and <em>data-plane programming</em>. Currently, I am a Principal Engineer working at the Huawei <a href="https://ai4netlab.github.io">AI4NET Datacom lab</a> in Paris (France) focusing on the integration of Deep Learning into traffic monitoring systems for <em>continuous learning</em> and <em>network automation</em>.</p>
<p>Previously, I was a research associate at Telefonica Research, and a Principal Engineer at Telefonica UK/O2, where I designed and deployed in production an ML product to predict daily customer satisfaction for 30M+ O2 customers using a variety of live network logs.</p>

          <br>
          <ul class="list-inline list-social-icons mb-0">
            
                <li class="list-inline-item">
                  <a href="mailto:mail@afinamore.io" data-toggle="tooltip" title="Mail" data-offset="0,10" rel="me">
                    <span class="fa-stack fa-lg">
                      <i class="fa fa-circle fa-stack-2x"></i>
                      
                      <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
                      
                    </span>
                  </a>
                </li>
            
                <li class="list-inline-item">
                  <a href="https://www.linkedin.com/in/alessandro-finamore-a741a45/" data-toggle="tooltip" title="LinkedIn" data-offset="0,10" rel="me">
                    <span class="fa-stack fa-lg">
                      <i class="fa fa-circle fa-stack-2x"></i>
                      
                      <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
                      
                    </span>
                  </a>
                </li>
            
                <li class="list-inline-item">
                  <a href="https://scholar.google.com/citations?user=PDm2yXEAAAAJ&amp;hl=en" data-toggle="tooltip" title="GoogleScholar" data-offset="0,10" rel="me">
                    <span class="fa-stack fa-lg">
                      <i class="fa fa-circle fa-stack-2x"></i>
                      
                      <span class="fab fa-scholar fa-stack-2x"></span>
                      
                    </span>
                  </a>
                </li>
            
                <li class="list-inline-item">
                  <a href="https://twitter.com/finale80" data-toggle="tooltip" title="Twitter" data-offset="0,10" rel="me">
                    <span class="fa-stack fa-lg">
                      <i class="fa fa-circle fa-stack-2x"></i>
                      
                      <i class="fab fa-brands fa-x-twitter fa-stack-1x fa-inverse"></i>
                      
                    </span>
                  </a>
                </li>
            
                <li class="list-inline-item">
                  <a href="https://github.com/finale80" data-toggle="tooltip" title="Github" data-offset="0,10" rel="me">
                    <span class="fa-stack fa-lg">
                      <i class="fa fa-circle fa-stack-2x"></i>
                      
                      <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                      
                    </span>
                  </a>
                </li>
            
          </ul>
        </div>
      </div>
    </section>

   
   
   
              
        <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="projects">
          <div class="my-auto">
            <h2 class="mb-5">Projects</h2>

            <div class="project-section">
                <table>
                <tr>
                <td>
                <a href="https://tcbenchstack.github.io/tcbench/"><img src="img/tcbench_logo.svg"/></a>
                </td>
                <td>
                A Machine Learning and Deep Learning modeling framework for Traffic Classification
                <br>
                <a href="https://github.com/tcbenchstack/tcbench">Github <i class="fab fa-github"></i></a>
                <a href="https://tcbenchstack.github.io/tcbench/">Documentation <i class="fas fa-book"></i></a>
                </td>
                <tr>
                </table>
            </div>

           </div>
        </section>


   
   
   
              
        <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="talks">
          <div class="my-auto">
            <h2 class="mb-5">Talks</h2>



            
              <div > 
                    <span class="middle pubs"> 
                        <span class="pubs-title">
                            
                            <a href="/pubs/keynote-TMA22-Jun2022.pdf">Taming the Data Divide to Enable AI-Driven Networks</a>
                            <a href="/pubs/keynote-TMA22-Jun2022.pdf" class="nostyle">
                                <span class="tag-box tag-ppt">SLIDES</span>
                            </a>
                            
                            
                                <span class="nostyle">
                                    <span class="tag-box tag-details" id="keynote-&lt;nil&gt;">details <i class="fas fa-chevron-down"></i></span>
                                </span>
                            
                        </span>
                        <div>IFIP Traffic Measurements and Analysis (TMA) &sdot; keynote &sdot; Jun, 2022</div>
                        
                        <div class="pubs-details" id="content-keynote-&lt;nil&gt;">
                            Artificial Intelligence (AI) is more and more penetrating networks design and operation. Yet, if/what we need to and how/where to integrate AI in networks is still largely a debate, arguably due to the fundamental need for effective sharing of data and measurements. In this talk we review the challenges surrounding this renewed “data divide” and discuss possible ways to mitigate them.
                        </div>
                        
                        <br>
                    </span>
              </div>
              
              <div > 
                    <span class="middle pubs"> 
                        <span class="pubs-title">
                            
                            <a href="/pubs/panel_MedComNet-Jun2021.pdf">Network Intelligence for the future</a>
                            <a href="/pubs/panel_MedComNet-Jun2021.pdf" class="nostyle">
                                <span class="tag-box tag-ppt">SLIDES</span>
                            </a>
                            
                            
                        </span>
                        <div>Mediterranean Communication and Computer Networking Conference (MedComNet) &sdot; panel &sdot; Jun, 2021</div>
                        
                        <br>
                    </span>
              </div>
              
           </div>
        </section>


   
   
        
      
      
      
            
            
      

              
        
        

        

        <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="research">
          <div class="my-auto">
            <h2 class="mb-5">Research</h2>

            <h3 class="mb-5">Recent publications</h3>
            
              

              <div > 
                <span class="pubs-year">2024</span>
                <ul>
                
                    <li class="pubs">
                    <span class="middle pubs"> 
                        <span class="pubs-title">
                            
                            <a href="/pubs/">Information Theoretic Text-to-Image Alignment</a>

                            <a href="/pubs/" class="nostyle">
                                <span class="tag-box tag-pdf">PDF</span>
                            </a>

                            

                            
                            <span class="nostyle">
                                <span class="tag-box tag-details" id="2024-0">details <i class="fas fa-chevron-down"></i></span>
                            </span>
                        </span>

                        <div class="pubs-authors">C. Wang, G. Franzese, A. Finamore, M. Gallo, P. Michiardi</div>

                        <span>arXiv (2405.20759)</span>
                            <div class="pubs-details" id="content-2024-0">
                                
                                <div style="text-align:right"><a href="https://arxiv.org/abs/2405.20759">preprint :: https://arxiv.org/abs/2405.20759</a></div>
                                <div>Diffusion models for Text-to-Image (T2I) conditional generation have seen tremendous success recently. Despite their success, accurately capturing user intentions with these models still requires a laborious trial and error process. This challenge is commonly identified as a model alignment problem, an issue that has attracted considerable attention by the research community. Instead of relying on fine-grained linguistic analyses of prompts, human annotation, or auxiliary vision-language models to steer image generation, in this work we present a novel method that relies on an information-theoretic alignment measure. In a nutshell, our method uses self-supervised fine-tuning and relies on point-wise mutual information between prompts and images to define a synthetic training set to induce model alignment. Our comparative analysis shows that our method is on-par or superior to the state-of-the-art, yet requires nothing but a pre-trained denoising network to estimate MI and a lightweight fine-tuning strategy.</div>
                                <hr>
                                <div>
                                    <span class="bibtex">@misc{AF:CORR24,</span>
                                    <span class="bibtex ml-10">title={Information Theoretic Text-to-Image Alignment},</span>
                                    <span class="bibtex ml-10">author={C. {Wang} and G. {Franzese} and A. {Finamore} and M. {Gallo} and P. {Michiardi}},</span>
                                    <span class="bibtex ml-10">year={2024},</span>
                                    
                                    
                                    
                                    
                                    
                                    
                                    <span class="bibtex ml-10">doi=https://doi.org/10.48550/arXiv.2405.20759,</span>
                                    <span class="bibtex ml-10">howpublished="https://afinamore.io/pubs/"</span>
                                    <span>}</span>
                                </div>
                            </div>
                    </span>
                    </li>
                
                    <li class="pubs">
                    <span class="middle pubs"> 
                        <span class="pubs-title">
                            
                            <a href="/pubs/PAM24_data-augmentation.pdf">Data Augmentation for Traffic Classification</a>

                            <a href="/pubs/PAM24_data-augmentation.pdf" class="nostyle">
                                <span class="tag-box tag-pdf">PDF</span>
                            </a>

                            

                            
                            <span class="nostyle">
                                <span class="tag-box tag-details" id="2024-1">details <i class="fas fa-chevron-down"></i></span>
                            </span>
                        </span>

                        <div class="pubs-authors">C. Wang, A. Finamore, P. Michiardi, M. Gallo, D. Rossi</div>

                        <span>Passive and Active Measurements (PAM)</span>
                            <div class="pubs-details" id="content-2024-1">
                                
                                <div style="text-align:right"><a href="https://arxiv.org/abs/2401.10754">preprint :: https://arxiv.org/abs/2401.10754</a></div>
                                <div>Data Augmentation (DA) -- enriching training data by adding synthetic samples -- is a technique widely adopted in Computer Vision (CV) and Natural Language Processing (NLP) tasks to improve models performance. Yet, DA has struggled to gain traction in networking contexts, particularly in Traffic Classification (TC) tasks. In this work, we fulfill this gap by benchmarking 18 augmentation functions applied to 3 TC datasets using packet time series as input representation and considering a variety of training conditions. Our results show that (i) DA can reap benefits previously unexplored, (ii) augmentations acting on time series sequence order and masking are better suited for TC than amplitude augmentations and (iii) basic models latent space analysis can help understanding the positive/negative effects of augmentations on classification performance.</div>
                                <hr>
                                <div>
                                    <span class="bibtex">@inproceeding{AF:PAM24,</span>
                                    <span class="bibtex ml-10">title={Toward Generative Data Augmentation for Traffic Classification},</span>
                                    <span class="bibtex ml-10">author={C. {Wang} and A. {Finamore} and P. {Michiardi} and M. {Gallo} and D. {Rossi}},</span>
                                    <span class="bibtex ml-10">year={2024},</span>
                                    
                                    <span class="bibtex ml-10">booktitle={Passive and Active Measurements (PAM)},</span>
                                    
                                    
                                    <span class="bibtex ml-10">location={Virtual},</span>
                                    
                                    
                                    
                                    
                                    
                                    <span class="bibtex ml-10">doi={10.1007/978-3-031-56249-5_7},</span>
                                    <span class="bibtex ml-10">howpublished="https://afinamore.io/pubs/PAM24_data-augmentation.pdf"</span>
                                    <span>}</span>
                                </div>
                            </div>
                    </span>
                    </li>
                
                </ul>

              </div>
              
              

              <div > 
                <span class="pubs-year">2023</span>
                <ul>
                
                    <li class="pubs">
                    <span class="middle pubs"> 
                        <span class="pubs-title">
                            
                            <a href="/pubs/CoNEXT23_sw_handcrafted_da.pdf">Toward Generative Data Augmentation for Traffic Classification</a>

                            <a href="/pubs/CoNEXT23_sw_handcrafted_da.pdf" class="nostyle">
                                <span class="tag-box tag-pdf">PDF</span>
                            </a>

                            

                            
                            <span class="nostyle">
                                <span class="tag-box tag-details" id="2023-0">details <i class="fas fa-chevron-down"></i></span>
                            </span>
                        </span>

                        <div class="pubs-authors">C.Wang, A. Finamore, P. Michiardi, M. Gallo, D. Rossi</div>

                        <span>Student workshop at ACM Conference on emerging Networking Experiments and Technologies (CoNEXT)</span>
                            <div class="pubs-details" id="content-2023-0">
                                
                                <div style="text-align:right"><a href="http://arxiv.org/abs/2310.13935">preprint :: http://arxiv.org/abs/2310.13935</a></div>
                                <div>Data Augmentation (DA)--augmenting training data with synthetic samples—is wildly adopted in Computer Vision (CV) to improve models performance. Conversely, DA has not been yet popularized in networking use cases, including Traffic Classification (TC). In this work, we present a preliminary study of 14 hand-crafted DAs applied on the MIRAGE19 dataset. Our results (i) show that DA can reap benefits previ- ously unexplored in TC and (ii) foster a research agenda on the use of generative models to automate DA design.</div>
                                <hr>
                                <div>
                                    <span class="bibtex">@inproceeding{AF:CoNEXT23,</span>
                                    <span class="bibtex ml-10">title={Toward Generative Data Augmentation for Traffic Classification},</span>
                                    <span class="bibtex ml-10">author={C. {Wang} and A. {Finamore} and P. {Michiardi} and M. {Gallo} and D. {Rossi}},</span>
                                    <span class="bibtex ml-10">year={2023},</span>
                                    
                                    <span class="bibtex ml-10">booktitle={Conference on emerging Networking Experiments and Technologies (CoNEXT), Student workshop},</span>
                                    
                                    
                                    <span class="bibtex ml-10">location={Paris, France},</span>
                                    
                                    
                                    
                                    
                                    
                                    <span class="bibtex ml-10">doi={},</span>
                                    <span class="bibtex ml-10">howpublished="https://afinamore.io/pubs/CoNEXT23_sw_handcrafted_da.pdf"</span>
                                    <span>}</span>
                                </div>
                            </div>
                    </span>
                    </li>
                
                    <li class="pubs">
                    <span class="middle pubs"> 
                        <span class="pubs-title">
                            
                            <a href="/pubs/IMC23_replication.pdf">Replication: Contrastive Learning and Data Augmentation in Traffic Classification Using a Flowpic Input Representation</a>

                            <a href="/pubs/IMC23_replication.pdf" class="nostyle">
                                <span class="tag-box tag-pdf">PDF</span>
                            </a>

                            
                                
                                <a href="/pubs/IMC23_replication_slides_long.pdf" class="nostyle">
                                    <span class="tag-box tag-ppt">SLIDES</span>
                                </a>
                            

                            
                            <span class="nostyle">
                                <span class="tag-box tag-details" id="2023-1">details <i class="fas fa-chevron-down"></i></span>
                            </span>
                        </span>

                        <div class="pubs-authors"></div>

                        <span>ACM Internet Measurement Conference (IMC)</span>
                            <div class="pubs-details" id="content-2023-1">
                                
                                <div style="text-align:right"><a href="https://dl.acm.org/doi/10.1145/3618257.3624820">https://dl.acm.org/doi/10.1145/3618257.3624820</a></div>
                                <div>The popularity of Deep Learning (DL), coupled with network traffic visibility reduction due to the increased adoption of HTTPS, QUIC and DNS-SEC, re-ignited interest towards Traffic Classification (TC). However, to tame the dependency from task-specific large labeled datasets we need to find better ways to learn representations that are valid across tasks. In this work we investigate this problem comparing transfer learning, meta-learning and contrastive learning against reference Machine Learning (ML) tree-based and monolithic DL models (16 methods total). Using two publicly available datasets, namely MIRAGE19 (40 classes) and AppClassNet (500 classes), we show that (i) using large datasets we can obtain more general representations, (ii) contrastive learning is the best methodology and (iii) meta-learning the worst one, and (iv) while ML tree-based cannot handle large tasks but fits well small tasks, by means of reusing learned representations, DL methods are reaching tree-based models performance also for small tasks.</div>
                                <hr>
                                <div>
                                    <span class="bibtex">@inproceeding{AF:IMC23,</span>
                                    <span class="bibtex ml-10">title={Replication: Contrastive Learning and Data Augmentation in Traffic Classification Using a Flowpic Input Representation},</span>
                                    <span class="bibtex ml-10">author=,</span>
                                    <span class="bibtex ml-10">year={2023},</span>
                                    
                                    <span class="bibtex ml-10">booktitle={Internet Measurement Conference (IMC)},</span>
                                    
                                    
                                    <span class="bibtex ml-10">location={Montreal, Canada},</span>
                                    
                                    
                                    
                                    
                                    
                                    <span class="bibtex ml-10">doi={},</span>
                                    <span class="bibtex ml-10">howpublished="https://afinamore.io/pubs/IMC23_replication.pdf"</span>
                                    <span>}</span>
                                </div>
                            </div>
                    </span>
                    </li>
                
                    <li class="pubs">
                    <span class="middle pubs"> 
                        <span class="pubs-title">
                            
                            <a href="/pubs/TMA23_manyorfew.pdf">Many or Few Samples? Comparing Transfer, Contrastive and Meta-Learning in Encrypted Traffic Classification</a>

                            <a href="/pubs/TMA23_manyorfew.pdf" class="nostyle">
                                <span class="tag-box tag-pdf">PDF</span>
                            </a>

                            

                            
                            <span class="nostyle">
                                <span class="tag-box tag-details" id="2023-2">details <i class="fas fa-chevron-down"></i></span>
                            </span>
                        </span>

                        <div class="pubs-authors">I. Guarino, C. Wang, A. Finamore, A. Pescape, D. Rossi</div>

                        <span>IEEE/IFIP Traffic Measurement and Analysis (TMA)</span>
                            <div class="pubs-details" id="content-2023-2">
                                
                                <div style="text-align:right"><a href="https://arxiv.org/abs/2305.12432">preprint :: https://arxiv.org/abs/2305.12432</a></div>
                                <div>The popularity of Deep Learning (DL), coupled with network traffic visibility reduction due to the increased adoption of HTTPS, QUIC and DNS-SEC, re-ignited interest towards Traffic Classification (TC). However, to tame the dependency from task-specific large labeled datasets we need to find better ways to learn representations that are valid across tasks. In this work we investigate this problem comparing transfer learning, meta-learning and contrastive learning against reference Machine Learning (ML) tree-based and monolithic DL models (16 methods total). Using two publicly available datasets, namely MIRAGE19 (40 classes) and AppClassNet (500 classes), we show that (i) using large datasets we can obtain more general representations, (ii) contrastive learning is the best methodology and (iii) meta-learning the worst one, and (iv) while ML tree-based cannot handle large tasks but fits well small tasks, by means of reusing learned representations, DL methods are reaching tree-based models performance also for small tasks.</div>
                                <hr>
                                <div>
                                    <span class="bibtex">@article{AF:TMA23,</span>
                                    <span class="bibtex ml-10">title={Many or Few Samples? Comparing Transfer, Contrastive and Meta-Learning in Encrypted Traffic Classification},</span>
                                    <span class="bibtex ml-10">author={I. {Guarino} and C. {Wang} and A. {Finamore} and A. {Pescape} and D. {Rossi}},</span>
                                    <span class="bibtex ml-10">year={2023},</span>
                                    
                                    <span class="bibtex ml-10">booktitle=Traffic Measurement and Analysis (TMA),</span>
                                    
                                    
                                    <span class="bibtex ml-10">location={Naples, Italy},</span>
                                    
                                    
                                    
                                    
                                    
                                    <span class="bibtex ml-10">doi={10.48550/arXiv.2305.12432},</span>
                                    <span class="bibtex ml-10">howpublished="https://afinamore.io/pubs/TMA23_manyorfew.pdf"</span>
                                    <span>}</span>
                                </div>
                            </div>
                    </span>
                    </li>
                
                    <li class="pubs">
                    <span class="middle pubs"> 
                        <span class="pubs-title">
                            
                            <a href="/pubs/PracticalDL23_mtl.pdf">&#34;It&#39;s a Match!&#34; -- A Benchmark of Task Affinity Scores for Joint Learning</a>

                            <a href="/pubs/PracticalDL23_mtl.pdf" class="nostyle">
                                <span class="tag-box tag-pdf">PDF</span>
                            </a>

                            

                            
                            <span class="nostyle">
                                <span class="tag-box tag-details" id="2023-3">details <i class="fas fa-chevron-down"></i></span>
                            </span>
                        </span>

                        <div class="pubs-authors">R. Azorin, M. Gallo, A. Finamore, D. Rossi, P. Michiardi</div>

                        <span>International Workshop on Practical Deep Learning in the Wild (PracticalDL) - colocated with AAAI</span>
                            <div class="pubs-details" id="content-2023-3">
                                
                                <div style="text-align:right"><a href="https://arxiv.org/abs/2301.02873">preprint :: https://arxiv.org/abs/2301.02873</a></div>
                                <div>While the promises of Multi-Task Learning (MTL) are attractive, characterizing the conditions of its success is still an open problem in Deep Learning. Some tasks may benefit from being learned together while others may be detrimental to one another. From a task perspective, grouping cooperative tasks while separating competing tasks is paramount to reap the benefits of MTL, i.e., reducing training and inference costs. Therefore, estimating task affinity for joint learning is a key endeavor. Recent work suggests that the training conditions themselves have a significant impact on the outcomes of MTL. Yet, the literature is lacking of a benchmark to assess the effectiveness of tasks affinity estimation techniques and their relation with actual MTL performance. In this paper, we take a first step in recovering this gap by (i) defining a set of affinity scores by both revisiting contributions from previous literature as well presenting new ones and (ii) benchmarking them on the Taskonomy dataset. Our empirical campaign reveals how, even in a small-scale scenario, task affinity scoring does not correlate well with actual MTL performance. Yet, some metrics can be more indicative than others.</div>
                                <hr>
                                <div>
                                    <span class="bibtex">@article{AF:PracticalDL23,</span>
                                    <span class="bibtex ml-10">title={&#34;It&#39;s a Match!&#34; -- A Benchmark of Task Affinity Scores for Joint Learning},</span>
                                    <span class="bibtex ml-10">author={R. {Azorin} and M. {Gallo} and A. {Finamore} and D. {Rossi} and P. {Michiardi}},</span>
                                    <span class="bibtex ml-10">year={2023},</span>
                                    
                                    <span class="bibtex ml-10">booktitle={International Workshop on Practical Deep Learning in the Wild (PracticalDL)},</span>
                                    
                                    
                                    <span class="bibtex ml-10">location={Washington, US},</span>
                                    
                                    
                                    
                                    
                                    
                                    <span class="bibtex ml-10">doi={10.48550/arXiv.2301.02873},</span>
                                    <span class="bibtex ml-10">howpublished="https://afinamore.io/pubs/PracticalDL23_mtl.pdf"</span>
                                    <span>}</span>
                                </div>
                            </div>
                    </span>
                    </li>
                
                </ul>

              </div>
              
              

              <div > 
                <span class="pubs-year">2022</span>
                <ul>
                
                    <li class="pubs">
                    <span class="middle pubs"> 
                        <span class="pubs-title">
                            
                            <a href="/pubs/INFOCOM22_approximate_key_caching.pdf">Accelerating Deep Learning Classification with Error-controlled Approximate-key Caching</a>

                            <a href="/pubs/INFOCOM22_approximate_key_caching.pdf" class="nostyle">
                                <span class="tag-box tag-pdf">PDF</span>
                            </a>

                            

                            
                            <span class="nostyle">
                                <span class="tag-box tag-details" id="2022-0">details <i class="fas fa-chevron-down"></i></span>
                            </span>
                        </span>

                        <div class="pubs-authors">A. Finamore, J. Roberts, M. Gallo, D. Rossi</div>

                        <span>IEEE International Conference on Computer Communications (INFOCOM)</span>
                            <div class="pubs-details" id="content-2022-0">
                                
                                <div style="text-align:right"><a href="https://arxiv.org/abs/2112.06671">preprint :: https://arxiv.org/abs/2112.06671</a></div>
                                <div>While Deep Learning (DL) technologies are a promising tool to solve networking problems that map to classification tasks, their computational complexity is still too high with respect to real-time traffic measurements requirements. To reduce the DL inference cost, we propose a novel caching paradigm, that we named approximate-key caching, which returns approximate results for lookups of selected input based on cached DL inference results. While approximate cache hits alleviate DL inference workload and increase the system throughput, they however introduce an approximation error. As such, we couple approximate-key caching with an error-correction principled algorithm, that we named auto-refresh. We analytically model our caching system performance for classic LRU and ideal caches, we perform a trace-driven evaluation of the expected performance, and we compare the benefits of our proposed approach with the state-of-the-art similarity caching -- testifying the practical interest of our proposal.</div>
                                <hr>
                                <div>
                                    <span class="bibtex">@inproceedings{AF:INFOCOM22,</span>
                                    <span class="bibtex ml-10">title={Accelerating Deep Learning Classification with Error-controlled Approximate-key Caching},</span>
                                    <span class="bibtex ml-10">author={A. {Finamore} and J. {Roberts} and M. {Gallo} and D. {Rossi}},</span>
                                    <span class="bibtex ml-10">year={2022},</span>
                                    
                                    <span class="bibtex ml-10">booktitle={IEEE International Conference on Computer Communications (INFOCOM)},</span>
                                    
                                    
                                    <span class="bibtex ml-10">location={Virtual Event},</span>
                                    
                                    
                                    
                                    
                                    
                                    <span class="bibtex ml-10">doi={10.1109/INFOCOM48880.2022.9796677},</span>
                                    <span class="bibtex ml-10">howpublished="https://afinamore.io/pubs/INFOCOM22_approximate_key_caching.pdf"</span>
                                    <span>}</span>
                                </div>
                            </div>
                    </span>
                    </li>
                
                    <li class="pubs">
                    <span class="middle pubs"> 
                        <span class="pubs-title">
                            
                            <a href="/pubs/HotNets22_representation.pdf">Towards a systematic multi-modal representation learning for network data</a>

                            <a href="/pubs/HotNets22_representation.pdf" class="nostyle">
                                <span class="tag-box tag-pdf">PDF</span>
                            </a>

                            

                            
                            <span class="nostyle">
                                <span class="tag-box tag-details" id="2022-1">details <i class="fas fa-chevron-down"></i></span>
                            </span>
                        </span>

                        <div class="pubs-authors">Z. B. Houidi, R. Azorin, M. Gallo, A. Finamore, D. Rossi</div>

                        <span>ACM Workshop on Hot Topics in Networks (HotNets)</span>
                            <div class="pubs-details" id="content-2022-1">
                                
                                <div style="text-align:right"><a href="https://dl.acm.org/doi/abs/10.1145/3563766.3564108">https://dl.acm.org/doi/abs/10.1145/3563766.3564108</a></div>
                                <div>Learning the right representations from complex input data is the key ability of successful machine learning (ML) models. The latter are often tailored to a specific data modality. For example, recurrent neural networks (RNNs) were designed having sequential data in mind, while convolutional neural networks (CNNs) were designed to exploit spatial correlation in images. Unlike computer vision (CV) and natural language processing (NLP), each of which targets a single well-defined modality, network ML problems often have a mixture of data modalities as input. Yet, instead of exploiting such abundance, practitioners tend to rely on sub-features thereof, reducing the problem to single modality for the sake of simplicity. In this paper, we advocate for exploiting all the modalities naturally present in network data. As a first step, we observe that network data systematically exhibits a mixture of quantities (e.g., measurements), and entities (e.g., IP addresses, names, etc.). Whereas the former are generally well exploited, the latter are often underused or poorly represented (e.g., with one-hot encoding). We propose to systematically leverage language models to learn entity representations, whenever significant sequences of such entities are historically observed. Through two diverse use-cases, we show that such entity encoding can benefit and naturally augment classic quantity-based features.</div>
                                <hr>
                                <div>
                                    <span class="bibtex">@inproceedings{AF:HotNets22,</span>
                                    <span class="bibtex ml-10">title={Towards a systematic multi-modal representation learning for network data},</span>
                                    <span class="bibtex ml-10">author={Z. B. {Houidi} and R. {Azorin} and M. {Gallo} and A. {Finamore} and D. {Rossi}},</span>
                                    <span class="bibtex ml-10">year={2022},</span>
                                    
                                    <span class="bibtex ml-10">booktitle={ACM Workshop on Hot Topics in Networks (HotNets)},</span>
                                    
                                    
                                    
                                    
                                    
                                    
                                    <span class="bibtex ml-10">doi={10.1145/3563766.3564108},</span>
                                    <span class="bibtex ml-10">howpublished="https://afinamore.io/pubs/HotNets22_representation.pdf"</span>
                                    <span>}</span>
                                </div>
                            </div>
                    </span>
                    </li>
                
                    <li class="pubs">
                    <span class="middle pubs"> 
                        <span class="pubs-title">
                            
                            <a href="/pubs/CCR22_appclassnet.pdf">AppClassNet: a commercial-grade dataset for application identification research</a>

                            <a href="/pubs/CCR22_appclassnet.pdf" class="nostyle">
                                <span class="tag-box tag-pdf">PDF</span>
                            </a>

                            

                            
                            <span class="nostyle">
                                <span class="tag-box tag-details" id="2022-2">details <i class="fas fa-chevron-down"></i></span>
                            </span>
                        </span>

                        <div class="pubs-authors">C. Wang, A. Finamore, L. Yang, K. Fauvel, D. Rossi</div>

                        <span>ACM Computer Communication Review (CCR)</span>
                            <div class="pubs-details" id="content-2022-2">
                                
                                <div style="text-align:right"><a href="https://dl.acm.org/doi/10.1145/3561954.3561958">https://dl.acm.org/doi/10.1145/3561954.3561958</a></div>
                                <div>The recent success of Artificial Intelligence (AI) is rooted into several concomitant factors, namely theoretical progress coupled with abundance of data and computing power. Large companies can take advantage of a deluge of data, typically withhold from the research community due to privacy or business sensitivity concerns, and this is particularly true for networking data. Therefore, the lack of high quality data is often recognized as one of the main factors currently limiting networking research from fully leveraging AI methodologies potential.  Following numerous requests we received from the scientific community, we release AppClassNet, a commercial-grade dataset for benchmarking traffic classification and management methodologies. AppClassNet is significantly larger than the datasets generally available to the academic community in terms of both the number of samples and classes, and reaches scales similar to the popular ImageNet dataset commonly used in computer vision literature. To avoid leaking user- and business-sensitive information, we opportunely anonymized the dataset, while empirically showing that it still represents a relevant benchmark for algorithmic research. In this paper, we describe the public dataset and our anonymization process. We hope that AppClassNet can be instrumental for other researchers to address more complex commercial-grade problems in the broad field of traffic classification and management.</div>
                                <hr>
                                <div>
                                    <span class="bibtex">@article{AF:CCR22,</span>
                                    <span class="bibtex ml-10">title={AppClassNet: a commercial-grade dataset for application identification research},</span>
                                    <span class="bibtex ml-10">author={C. {Wang} and A. {Finamore} and L. {Yang} and K. {Fauvel} and D. {Rossi}},</span>
                                    <span class="bibtex ml-10">year={2022},</span>
                                    
                                    <span class="bibtex ml-10">booktitle={ACM Computer Communication Review (CCR)},</span>
                                    
                                    
                                    
                                    
                                    
                                    
                                    <span class="bibtex ml-10">doi={10.1145/3561954.3561958},</span>
                                    <span class="bibtex ml-10">howpublished="https://afinamore.io/pubs/CCR22_appclassnet.pdf"</span>
                                    <span>}</span>
                                </div>
                            </div>
                    </span>
                    </li>
                
                </ul>

              </div>
              
                
              
              <a href="/research" class="pubs-showmore">full list</a>
              
           </div>

            <h3 class="mb-5">TPC Member</h3>
            <ul>
                <li>IMC: 2025, 2024</li>
                <li>CoNEXT: 2025, 2016</li>
                <li>PAM: 2025, 2024, 2023, 2022</li>
                <li>TMA: 2023, 2022</li>
            </ul>

            <h3 class="mb-5">Awards</h3>
            <ul>
                <li>IMC 2024, Best reviewer</li>
                <li>TMA 2023, Best reviewer</li>
                <li>CoNEXT 2013, Best short paper</li>
            <ul>
        </section>


   
   
   
       <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="experience">
  <div class="my-auto" id="experience-content">
    <h2 class="mb-5">Experience</h2>
    
        <div class="resume-item d-flex flex-column flex-md-row mb-5">
          <div class="resume-content mr-auto">
            <h3 class="mb-0">Principal Engineer</h3>
            <div class="subheading mb-3">HUAWEI · Paris, France</div>
            <p>My current role is at the cross-over between BigData, networks, and AI. I work in the Huawei DataCom R&amp;D AI team focused on integrating AI in data-plane programming, distributed telemetry and other network monitoring solutions for the Huawei DataCom product line. In particular, I&rsquo;m leading the research related to traffic classification with an emphasis towards continual learning (e.g., incremental learning and few-shot learning) and data augmentation (e.g., self-supervision).  I&rsquo;m also responsible for the design and prototype of next-generation network probes which can take advantage of ML/DL via advanced GPU/TPU cards (e.g., Huawei Ascend 310 / 910) to compute advanced network analytics.</p>
          </div>
          <div class="resume-date text-md-right">
            <span class="text-primary">September 2019 - Present</span>
          </div>
        </div>
    
        <div class="resume-item d-flex flex-column flex-md-row mb-5">
          <div class="resume-content mr-auto">
            <h3 class="mb-0">Principal Engineer</h3>
            <div class="subheading mb-3">Telefonica UK / O2 · London, UK</div>
            <p>Started as research experiment and later graduated to product, I was leading the design and development of BigData analytics (using Apache Spark) and ML applications (using Keras, and scikit-lear). I took advantage of a large on-premise Hadoop cluster (250+ nodes) where data collected from different network core monitoring elements were stored, to create insights about users quality of experience (QoE), that were used to model &gt;30M customer satisfaction. I&rsquo;ve been responsible for the design, implementation, and operation of the whole pipeline (analytics+modeling) which has been successfully used internally. In parallel, I was still part of the research community (in particular related to traffic analysis), with different collaborations with universities and other research centres.</p>
          </div>
          <div class="resume-date text-md-right">
            <span class="text-primary">January 2018 - September 2019</span>
          </div>
        </div>
    
        <div class="resume-item d-flex flex-column flex-md-row mb-5">
          <div class="resume-content mr-auto">
            <h3 class="mb-0">Research Associate</h3>
            <div class="subheading mb-3">Telefonica Research · Barcelona, Spain</div>
            <p>I worked on research projects related to mobile network analytics spanning from traffic encryption (e.g., HTTP2 adoption and performance), to users quality of experience (e.g., mobile critical path analysis) and users behavior (e.g., users mobility). I also collaborated with different operational business within Telefonica global (e.g., O2/UK, Movistar/Peru, Movistar/Argentina) across different projects related to network analytics and bigdata (e.g., use radio tower KPIs to understand users experience).</p>
          </div>
          <div class="resume-date text-md-right">
            <span class="text-primary">December 2014 - January 2018</span>
          </div>
        </div>
    
        <div class="resume-item d-flex flex-column flex-md-row mb-5">
          <div class="resume-content mr-auto">
            <h3 class="mb-0">Visiting Scholar</h3>
            <div class="subheading mb-3">Narus Inc · Sunnyvale, CA, U.S</div>
            <p>I worked on developing novel techniques for identifying and dissecting network traffic generated by malware executable, rootkit applications, and more general mobile/host traffic behaviors. The techniques developed lead to discovery of security issues actually exploited in the wild.  I worked on developing novel techniques for identifying and dissecting network traffic generated by malware executable, rootkit applications, and more general mobile/host traffic behaviors. The techniques developed lead to discovery of security issues actually exploited in the wild.</p>
          </div>
          <div class="resume-date text-md-right">
            <span class="text-primary">Oct 2013 - September 2014</span>
          </div>
        </div>
    
        <div class="resume-item d-flex flex-column flex-md-row mb-5">
          <div class="resume-content mr-auto">
            <h3 class="mb-0">Internship</h3>
            <div class="subheading mb-3">Telefonica I+D · Barcelona, Spain</div>
            <p>I worked on a project to extract network analytics from a country-scale dataset by means of an Hadoop Cluster. The work lead to publication of one of the first studies related to the mobile ads ecosystem. I worked on a project to extract network analytics from a country-scale dataset by means of an Hadoop Cluster. The work lead to publication of one of the first studies related to the mobile ads ecosystem.</p>
          </div>
          <div class="resume-date text-md-right">
            <span class="text-primary">January 2012 - April 2014</span>
          </div>
        </div>
    
        <div class="resume-item d-flex flex-column flex-md-row mb-5">
          <div class="resume-content mr-auto">
            <h3 class="mb-0">Visiting PhD Student</h3>
            <div class="subheading mb-3">Purdue University · Lafayette, Indiana, U.S.</div>
            <p>I worked on a research project related to understanding the YouTube CDN by means of data gathered from passive network probes we deployed in Italy and and Poland ISPs. We have been the first to uncover YouTube CDN dynamics and (at the time) the aggressive buffering of the YouTube player.  I worked on a research project related to understanding the YouTube CDN by means of data gathered from passive network probes we deployed in Italy and and Poland ISPs. We have been the first to uncover YouTube CDN dynamics and (at the time) the aggressive buffering of the YouTube player.</p>
          </div>
          <div class="resume-date text-md-right">
            <span class="text-primary">September 2010 - May 2011</span>
          </div>
        </div>
    
  </div>

</section>

   
   
               <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="education">
          <div class="my-auto">
            <h2 class="mb-5">Education</h2>
            
              <div class="resume-item d-flex flex-column flex-md-row mb-5">
                <div class="resume-content mr-auto">
                  <h3 class="mb-0">Politecnico di Torino</h3>
                  <div class="subheading mb-3">Ph.D. in Electronics and Telecommunication Engineering.</div>
                  <div></div>
                  <p>
                </div>
                <div class="resume-date text-md-right">
                  <span class="text-primary">2008 - 2012</span>
                </div>
              </div>
              
              <div class="resume-item d-flex flex-column flex-md-row mb-5">
                <div class="resume-content mr-auto">
                  <h3 class="mb-0">Politecnico di Torino</h3>
                  <div class="subheading mb-3">Bachelor of Science in Computer Engineering (110/110)</div>
                  <div></div>
                  <p>
                </div>
                <div class="resume-date text-md-right">
                  <span class="text-primary">2005 - 2008</span>
                </div>
              </div>
              
              <div class="resume-item d-flex flex-column flex-md-row mb-5">
                <div class="resume-content mr-auto">
                  <h3 class="mb-0">Politecnico di Torino</h3>
                  <div class="subheading mb-3">Master of Science in Computer Engineering (110L/110)</div>
                  <div></div>
                  <p>
                </div>
                <div class="resume-date text-md-right">
                  <span class="text-primary">2001 - 2005</span>
                </div>
              </div>
              
           </div>
        </section>

   




    
  </div>
  
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx" crossorigin="anonymous"></script>

  
  <script async src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js" integrity="sha512-0QbL0ph8Tc8g5bLhfVzSqxe9GERORsKhIn1IrpxDAgUsbBGz/V7iSav2zzW325XGd1OMLdL4UiqRJj702IeqnQ==" crossorigin="anonymous"></script>
  
  
  <script async src="/js/pubs.js"></script>
  <script async src="/js/navbar.js"></script>


  

  
</body>
</html>
